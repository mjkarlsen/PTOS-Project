---
title: "Predicting Fasciotomy"
output: html_notebook
---

```{r}
pacman::p_load(doParallel, 
               tensorflow)

install_tensorflow()
```



```{r creating_upsample_model}
patient_list <- patient_periods %>%
  filter.(
    fltr_procedure == T,
    sex != "U"
    # peds_adult_flag == "Peds"
  ) %>%
  select.(
    id,
    fltr_fasciotomy,
    peds_adult_flag,
    fltr_procedure,
    sex,
    race,
    age_in_yrs,
    injury_desc,
    patient_county,
    cause_of_injury_e_code
  ) %>%
  distinct.()


fx_proc_df <- trans_full_df %>%
  filter.(data_source %in% c(
    "diagnosis",
    "procedure",
    "complication"
  )) %>%
  mutate.(data_source = fct_relevel(data_source, c(
    "diagnosis",
    "procedure",
    "complication"
  ))) %>%
  inner_join.(patient_list, by = "id") %>%
  arrange.(id, date, data_source, time) %>%
  mutate.(index = seq_len(.N), by = id) %>%
  mutate.(fltr_fasciotomy = as_factor(fltr_fasciotomy)) %>%
  select.(index, id, everything.())

clean_df <- fx_proc_df


fx_proc_df <- fx_proc_df %>%
  filter.(
    chapter_desc != "Extremity Compartment Syndrome (not present on admission)",
    code_cd != "83.14"
  ) %>%
  select.(
    # id,
    # index,
    fltr_fasciotomy,
    code_desc,
    subchapter_desc,
    sex,
    race,
    injury_desc,
    age_in_yrs,
    cause_of_injury_e_code
  ) %>%
  as_tibble() %>%
  na.omit()

```

## Upsample Data

```{r randomforest_model}

set.seed(123)
train_test_split <- rsample::initial_split(data = fx_proc_df, 
                                           strata = fltr_fasciotomy)


train_df <- train_test_split %>% training() 
test_df  <- train_test_split %>% testing()



data_balanced_over <- ovun.sample(fltr_fasciotomy ~ .,  #-id -index,
                                  data = train_df,
                                  method = "both",
                                  p = 0.5,
                                  N = 10000,
                                  seed = 123)$data

# table(data_balanced_over$fltr_fasciotomy)

rf_recipe <- 
  recipe(fltr_fasciotomy ~ ., data = data_balanced_over) %>%
  update_role(fltr_fasciotomy, new_role = "outcome") %>% 
  # update_role(id, index, new_role = "ID COL") %>% 
  step_other(all_predictors(), threshold = 0.001) %>%
  # step_unknown(all_nominal()) %>% 
  step_dummy(all_predictors(), -all_outcomes()) %>% 
  step_zv(all_nominal()) %>% 
  step_naomit(all_predictors()) %>% 
  prep()


train_data <- juice(rf_recipe)

test_data <- bake(rf_recipe, test_df)

```


```{r message=FALSE, warning=FALSE}

## RANDOM FOREST
rf_all_model <- rand_forest() %>%
  set_mode("classification") %>%
  set_engine("ranger", verbose = TRUE, importance = "impurity", num.threads = 6) %>%
  fit(fltr_fasciotomy ~ ., data = train_data)

# XGBoost on important words only
xgb_model <- boost_tree(mtry = .7,
                        trees = 200,
                        learn_rate = .02,
                        loss_reduction = 0,
                        tree_depth = (ncol(train_data) - 1),
                        sample_size = 1) %>%
  set_mode("classification") %>%
  set_engine("xgboost", nthread = 6) %>%
  fit(fltr_fasciotomy ~ ., data = train_data)

## LOGISTIC REGRESSION
glm_model <-
  logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(fltr_fasciotomy ~ ., data = train_data)

## NEURAL NETWORK
set.seed(57974)
nnet_model <- mlp(epochs = 20, hidden_units = 5, dropout = 0.1) %>%
  set_mode("classification") %>% 
  # Also set engine-specific `verbose` argument to prevent logging the results: 
  set_engine("keras", verbose = 0) %>%
  fit(fltr_fasciotomy ~ ., data = train_data)

## RIDGE
ridge_model <- logistic_reg(penalty = tune(), mixture = tune()) %>% # Specify hyperparameters to "tune"
  set_engine("glmnet")

best_parameters <- ridge_model %>%
  tune_grid(fltr_fasciotomy ~ .,
            resamples = vfold_cv(train_data, 5), # Specify folds
            grid = expand_grid(penalty = c(0, .1, 1, 10, 1000), # Specify grid to tune
                               mixture = c(0, .2, .4, .8))) %>%
  select_best(metric = "roc_auc") # Extract best parameters based on a metric


ridge_model <- ridge_model %>%
  finalize_model(best_parameters) %>% # Update model with best parameters
  fit(fltr_fasciotomy ~ ., data = train_data) # Fit model using best parameters

## XGB Using RIDGE Parameters
new_xgb <- boost_tree() %>%
  set_mode("classification") %>%
  set_engine("xgboost", nthread = 6) %>% 
  finalize_model(best_parameters) %>% # Update model with best parameters
  fit(fltr_fasciotomy ~ ., data = train_data) # Fit model using best parameters

```


```{r}
# Get predictions
test_pred_df <- test_data %>%
  select(fltr_fasciotomy) %>%
  bind_cols(predict(rf_all_model, test_data) %>% rename(rf_all = .pred_class),
            predict(xgb_model, test_data) %>% rename(xgb = .pred_class), 
            predict(glm_model, test_data) %>% rename(glm = .pred_class), 
            predict(nnet_model, test_data) %>% rename(nnet = .pred_class),
            predict(ridge_model, test_data) %>% rename(ridge = .pred_class),
            predict(new_xgb, test_data) %>% rename(new_xgb = .pred_class)
            )

# Check accuracy
test_pred_df %>%
  pivot_longer(-fltr_fasciotomy, names_to = "model", values_to = "pred") %>%
  group_by(model) %>%
  summarize(true_positive = recall_vec(fltr_fasciotomy, pred),
            false_positive = 1 - specificity_vec(fltr_fasciotomy, pred),
            precision = precision_vec(fltr_fasciotomy, pred))
```

```{r}
# Apply predictions
predictions_df <- clean_df %>%
  filter(!is.na(fltr_fasciotomy)) %>%
  select(id, fltr_fasciotomy) %>%
  bind_cols(predict(xgb_model, unknown_df) %>% rename(pred = .pred_class))

predictions_df
```

```{r}
vip::vip(xgb_model$fit,num_features = 15)
```


```{r}
xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune(),                       ## step size
) %>% 
  set_engine("xgboost", nthread = 6) %>% 
  set_mode("classification")

xgb_spec


xgb_grid <- grid_regular(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), train_data),
  learn_rate()
)

xgb_grid


xgb_wf <- workflow() %>%
  add_formula(fltr_fasciotomy ~  .) %>%
  add_model(xgb_spec)

xgb_wf


set.seed(123)
vb_folds <- vfold_cv(train_data, v = 5, strata = fltr_fasciotomy)

vb_folds


doParallel::registerDoParallel()

set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

xgb_res


collect_metrics(xgb_res)


xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")


show_best(xgb_res, "roc_auc")

best_auc <- select_best(xgb_res, "roc_auc")
best_auc


final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb


final_xgb %>%
  fit(data = train_data) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")


final_res <- last_fit(final_xgb, train_test_split)

collect_metrics(final_res)


final_res %>%
  collect_predictions() %>%
  roc_curve(fltr_fasciotomy, .pred_win) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )


# doParallel::stopImplicitCluster()

```



```{r}

# Find important words
importance_df <- rf_all_model$fit$variable.importance %>%
  enframe(name = "variable", value = "importance") %>%
  mutate(importance = rescale(importance)) %>%
  mutate_if(is.double, ~round(.x, 4)) %>%
  arrange(-importance)

# Get vec of important words
variable_list <- importance_df %>%
  filter(importance > .02) %>%
  arrange(importance) %>%
  pull(variable)

vip::vip(rf_all_model$fit, num_features = 15)
```